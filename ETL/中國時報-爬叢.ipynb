{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1.當日的新聞(標題 類別 日期 新聞連結 報社 )   OK    20163/19\n",
    "#2.單一新聞的 (內文 關鍵字 點擊率)            OK    2016/3/22\n",
    "#3.單頁寫入檔案 (json) (1+2+3)               OK    2016/3/23\n",
    "#4.焦點新聞 \"總分頁\" 結算  \"分頁\" (當日)      OK    2016/3/22\n",
    "#4-1.焦點新聞 \"總分頁\" 結算  \"分頁\" (多日)    \n",
    "#5.分類+存檔案(在本機端分類新聞類別的資料夾)\n",
    "# 6.total pages    試著找 2013-2016 總篇數  OK    2016/3/22\n",
    "#分類(在本機端分類新聞類別的資料夾)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.當日的新聞(ex,類別→中國時報)(標題 類別 日期 新聞連結 報社) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "url='http://www.chinatimes.com/history-by-date/2016-03-23-2601'\n",
    "res=requests.get(url)\n",
    "soup=BS(res.text)\n",
    "#print soup\n",
    "mainPage='http://www.chinatimes.com/'\n",
    "lis=soup.select('.listRight li')\n",
    "how_many_new_in_page=len(lis)\n",
    "for li in lis:\n",
    "    title=li.select('h2')[0].text.strip() #list→unicode  加了[0]→'bs4.element.Tag' 可顯示中文字\n",
    "    category=li.select('.kindOf a')[0].text.strip() #類別\n",
    "    Data2=soup.select('.page_index li')[1]\n",
    "    Data1=Data2.select('h6 a')[0]['href'][0:].split('-')\n",
    "    Data=Data1[0]+Data1[1]+Data1[2]             #yyyymmdd  日期    \n",
    "    newspaper_office='中國時報'                  #報社\n",
    "    page1=li.select('h2 a')[0]['href'][0:]      #新聞連結，進入到內文的階段\n",
    "    each_new_page_url=mainPage+page1 \n",
    "    second_url=requests.get(each_new_page_url)\n",
    "    second_usoup=BS(second_url.text)  \n",
    "    print second_usoup\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.單一新聞的 (內文 關鍵字 點擊率)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "比利時首都布魯塞爾22日驚爆連環恐怖攻擊_札范登（Zaventem）國際機場和臨近歐盟總部的梅爾比克（Maelbeek）地鐵站先後發生3起爆炸_總共至少造成34人死亡、逾百人受傷_恐怖聖戰組織「伊斯蘭國」（IS）透過掛鉤的阿瑪克通訊社坦承犯案_恐攻發生數小時後_武裝警察在市中心逮捕了2名男子_地點距梅爾比克地鐵站僅1公里多_還有1名男子疑似在機場外被戴上手銬_另外_當局在爆炸現場查獲炸藥帶和槍枝_擔心可能有涉及爆炸攻擊的嫌犯仍然在逃_疑似主嫌 現場活逮比利時警方18日在布魯塞爾郊區莫倫比克一棟公寓_逮捕巴黎恐攻主嫌之一阿布岱斯蘭_該國內政部長詹彭曾警告_阿布岱斯蘭落網可能招致歐洲境內其它恐怖網絡進行報復性攻擊_他的擔憂如今成真_比利時遭逢血腥恐攻_舉國悲愴_總理米歇爾向國人信心喊話說：「這是我們國家黑暗的一刻_我們一直擔憂遭受恐攻_如今它真的發生了_值此艱難時刻_我們必須團結一致迎接挑戰_」自殺炸彈 攻擊機場比利時當地時間上午近8時_札范登機場出境大廳一家航空公司報到櫃台附近_接連發生2起爆炸_至少造成14死、81人受傷_事發後_機場取消所有航班_宣布無限期關閉_當局說_其中一起爆炸是自殺炸彈攻擊_警方在當場斃命的炸彈客身旁發現一把攻擊步槍_還好他來不及持槍濫射_否則後果不堪設想_此外_軍警也引爆機場內另1個可疑包裹_目擊者說_2起爆炸間隔約15秒_人們驚嚇逃竄_出境大廳滿地是血_航廈內的影像顯示_現場滿目瘡痍_天花板碎落地面_行李箱也散落一地_機場外監視器畫面則可看到_航廈冒出滾滾濃煙_大批旅客驚惶逃離現場_歐盟總部 地鐵引爆約莫1小時後_市中心歐盟總部附近的梅爾比克地鐵站_也驚傳爆炸巨響_一列地鐵正要駛離前_車廂內炸裂物被引爆_時值上班尖峰時段_至少造成20人死亡、106人受傷_其中17人傷勢嚴重_電視畫面顯示_地鐵入口冒出黑煙_有傷者渾身是血_一名覆蓋黃色裹屍袋的死者被抬出_比利時內政部長詹彭認定這是連環恐攻_下令將防恐警戒層級_從3級提升到最高的4級_並緊急關閉布魯塞爾地鐵、電車和巴士等大眾運輸系統_此外_德國法蘭克福機場、巴黎戴高樂機場和倫敦蓋維克機場也都加強警戒_歐洲之星也暫停往來倫敦與布魯塞爾的高速火車服務_英美法國 升高警戒另外_歐盟宣布所轄機構進入橘色警戒_取消所有會議_嚴控人員出入_英國和法國和紐約市府也隨即提升安全警戒_美國航空發聲明澄清_炸裂物並非出現在該公司報到櫃台附近_但美國駐比利時大使仍呼籲_美國公民應避免外出和搭乘公共運輸工具_(中國時報)\n"
     ]
    }
   ],
   "source": [
    "each_new_page_url='http://www.chinatimes.com/newspapers/20160323000482-260102'\n",
    "second_url=requests.get(each_new_page_url)\n",
    "second_usoup=BS(second_url.text)\n",
    "hits=second_usoup.findAll('div',{'class':'art_click clear-fix'})[0]\n",
    "number_of_hits = int(hits.select('span')[1].text)            #點擊率\n",
    "all_p_tags=second_usoup.select('.clear-fix p')[:-6]\n",
    "content_list=[]\n",
    "\n",
    "\n",
    "for p_tag in all_p_tags:\n",
    "    p_tag1=str(p_tag).replace(\"，\",\"_\")\n",
    "    p_tag2=str(p_tag1).replace(\"。\",\"_\")\n",
    "    p_tag3=str(p_tag2).replace(\"<p>\",\"\")\n",
    "    p_tag4=str(p_tag3).replace(\"</p>\",\"\")\n",
    "    #print p_tag4                                          #內文(已將 \"，\" & \"。\"轉換)\n",
    "    content_list.append(p_tag4)                            #將全部 標籤 P寫入 list中\n",
    "\n",
    "\n",
    "separator=''\n",
    "joined=separator.join(content_list)                        #將 list [i] 以 \"\" 合併     |獨家秘技!!!!| \n",
    "#print joined\n",
    "\n",
    "\n",
    "keyword=second_usoup.select('.a_k a')\n",
    "i=0\n",
    "for kwords in range(len(keyword)):\n",
    "    keyword=second_usoup.select('.a_k a')[i]\n",
    "    #print keyword.text                                       #此頁新聞的關鍵字\n",
    "    i=i+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***3.單頁寫入檔案 (json)***   (1+2+3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BIG DATA\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "#with open('C:/Users/BIG DATA/pythonETL/project/news/newOneDay.txt' , 'w') as f:\n",
    "    #f.write(p_tag4+\"\\n\")\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import json\n",
    "import ast  #轉換成json需要套件\n",
    "\n",
    "url='http://www.chinatimes.com/history-by-date/2016-03-23-2601'\n",
    "res=requests.get(url)\n",
    "soup=BS(res.text)\n",
    "#print soup\n",
    "mainPage='http://www.chinatimes.com/'\n",
    "lis=soup.select('.listRight li')\n",
    "how_many_new_in_page=len(lis)\n",
    "l=0\n",
    "\n",
    "all_json=''\n",
    "for li in lis:\n",
    "    l=l+1\n",
    "    if l<11:   #單頁報導要寫入幾天 \n",
    "        title=li.select('h2')[0].text.strip() #list→unicode  加了[0]→'bs4.element.Tag' 可顯示中文字\n",
    "        #print title\n",
    "        category=li.select('.kindOf a')[0].text.strip() #類別\n",
    "        Data2=soup.select('.page_index li')[1]\n",
    "        Data1=Data2.select('h6 a')[0]['href'][0:].split('-')\n",
    "        Data=Data1[0]+Data1[1]+Data1[2]              #yyyymmdd  日期    \n",
    "        newspaper_office='中國時報'                  #報社\n",
    "        page1=li.select('h2 a')[0]['href'][0:]      #新聞連結，進入到內文的階段\n",
    "        each_new_page_url=mainPage+page1 \n",
    "        second_url=requests.get(each_new_page_url)\n",
    "        second_usoup=BS(second_url.text)  \n",
    "        second_url=requests.get(each_new_page_url)\n",
    "        second_usoup=BS(second_url.text)\n",
    "        hits=second_usoup.findAll('div',{'class':'art_click clear-fix'})[0]\n",
    "        number_of_hits = int(hits.select('span')[1].text)            #點擊率\n",
    "        all_p_tags=second_usoup.select('.clear-fix p')[:-6]\n",
    "        content_list=[]\n",
    "        for p_tag in all_p_tags:\n",
    "            p_tag1=str(p_tag).replace(\"，\",\"_\")\n",
    "            p_tag2=str(p_tag1).replace(\"。\",\"_\")\n",
    "            p_tag3=str(p_tag2).replace(\"<p>\",\"\")\n",
    "            p_tag4=str(p_tag3).replace(\"</p>\",\"\")\n",
    "            #print p_tag4                                          #內文(已將 \"，\" & \"。\"轉換)\n",
    "            content_list.append(p_tag4)                            #將全部 標籤 P寫入 list中\n",
    "        separator=''\n",
    "        joined_content=separator.join(content_list)              #將 list [i] 以 \"\" 合併，得到全文內容    |獨家秘技!!!!| \n",
    "        #print joined_content\n",
    "        keyword=second_usoup.select('.a_k a')\n",
    "        i=0\n",
    "        kwords_list=[]\n",
    "        for kwords in range(len(keyword)):\n",
    "            keyword=second_usoup.select('.a_k a')[i]\n",
    "            #print keyword.text                                       #此頁新聞的關鍵字\n",
    "            i=i+1\n",
    "            kwords_list.append(keyword.text)\n",
    "        separator='_'    \n",
    "        joined_kwords=separator.join(kwords_list)               #將 list [i] 以 \"\" 合併，得到關鍵字全部內容 \n",
    "        #print joined_kwords\n",
    "        \n",
    "        m = {\n",
    "             'url':each_new_page_url,'title':title,'sort':category,\\\n",
    "             'content':joined_content,'time':Data,'people':number_of_hits,\\\n",
    "             'keywords':joined_kwords,'newspaper_office':'中國時報'\n",
    "             }                                               #填入json格式dict\n",
    "        \n",
    "        all_json+=str(m)+','            #dict不可以先加 所以先變STR相加   \n",
    "            \n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=eval(all_json)                       #AST套件 轉換 all_json 成 json\n",
    "d1 =  json.dumps(a)\n",
    "d =  json.loads(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'dict'>\n",
      "<type 'str'>\n",
      "<type 'tuple'>\n",
      "<type 'str'>\n",
      "<type 'list'>\n"
     ]
    }
   ],
   "source": [
    "print type(m)\n",
    "print type(all_json)\n",
    "print type(a)\n",
    "print type(d1)\n",
    "print type(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "布魯塞爾恐攻》比利時恐襲34死 布魯塞爾癱瘓 焦點要聞 20160323 31327 比利時首都布魯塞爾22日驚爆連環恐怖攻擊_札范登（Zaventem）國際機場和臨近歐盟總部的梅爾比克（Maelbeek）地鐵站先後發生3起爆炸_總共至少造成34人死亡、逾百人受傷_恐怖聖戰組織「伊斯蘭國」（IS）透過掛鉤的阿瑪克通訊社坦承犯案_恐攻發生數小時後_武裝警察在市中心逮捕了2名男子_地點距梅爾比克地鐵站僅1公里多_還有1名男子疑似在機場外被戴上手銬_另外_當局在爆炸現場查獲炸藥帶和槍枝_擔心可能有涉及爆炸攻擊的嫌犯仍然在逃_疑似主嫌 現場活逮比利時警方18日在布魯塞爾郊區莫倫比克一棟公寓_逮捕巴黎恐攻主嫌之一阿布岱斯蘭_該國內政部長詹彭曾警告_阿布岱斯蘭落網可能招致歐洲境內其它恐怖網絡進行報復性攻擊_他的擔憂如今成真_比利時遭逢血腥恐攻_舉國悲愴_總理米歇爾向國人信心喊話說：「這是我們國家黑暗的一刻_我們一直擔憂遭受恐攻_如今它真的發生了_值此艱難時刻_我們必須團結一致迎接挑戰_」自殺炸彈 攻擊機場比利時當地時間上午近8時_札范登機場出境大廳一家航空公司報到櫃台附近_接連發生2起爆炸_至少造成14死、81人受傷_事發後_機場取消所有航班_宣布無限期關閉_當局說_其中一起爆炸是自殺炸彈攻擊_警方在當場斃命的炸彈客身旁發現一把攻擊步槍_還好他來不及持槍濫射_否則後果不堪設想_此外_軍警也引爆機場內另1個可疑包裹_目擊者說_2起爆炸間隔約15秒_人們驚嚇逃竄_出境大廳滿地是血_航廈內的影像顯示_現場滿目瘡痍_天花板碎落地面_行李箱也散落一地_機場外監視器畫面則可看到_航廈冒出滾滾濃煙_大批旅客驚惶逃離現場_歐盟總部 地鐵引爆約莫1小時後_市中心歐盟總部附近的梅爾比克地鐵站_也驚傳爆炸巨響_一列地鐵正要駛離前_車廂內炸裂物被引爆_時值上班尖峰時段_至少造成20人死亡、106人受傷_其中17人傷勢嚴重_電視畫面顯示_地鐵入口冒出黑煙_有傷者渾身是血_一名覆蓋黃色裹屍袋的死者被抬出_比利時內政部長詹彭認定這是連環恐攻_下令將防恐警戒層級_從3級提升到最高的4級_並緊急關閉布魯塞爾地鐵、電車和巴士等大眾運輸系統_此外_德國法蘭克福機場、巴黎戴高樂機場和倫敦蓋維克機場也都加強警戒_歐洲之星也暫停往來倫敦與布魯塞爾的高速火車服務_英美法國 升高警戒另外_歐盟宣布所轄機構進入橘色警戒_取消所有會議_嚴控人員出入_英國和法國和紐約市府也隨即提升安全警戒_美國航空發聲明澄清_炸裂物並非出現在該公司報到櫃台附近_但美國駐比利時大使仍呼籲_美國公民應避免外出和搭乘公共運輸工具_(中國時報)\n",
      "布魯塞爾恐攻》巴黎恐攻共犯遭逮 IS連環「爆」復 焦點要聞 20160323 6567 比利時首都布魯塞爾22日發生連環爆炸案_外界揣測_由於巴黎恐攻事件唯一倖存的法國籍共犯阿布岱斯蘭18日在布魯塞爾遭活逮_這次應該是「伊斯蘭國」（IS）對此採取的報復行動_阿布岱斯蘭遭活捉_不僅對調查巴黎恐攻案是重大突破_對了解歐洲地區從2004年開始持續受到IS宣稱發動的武力襲擊內情_也將有所助益_應訊透露從比國開炸比利時外長雷恩德斯曾於20日表示_阿布岱斯蘭落網後應訊透露_曾計畫從布魯塞爾尋找目標_發動更多的恐怖攻擊行動_據稱_阿布岱斯蘭在他的第一個應訊聲明中_透露了展開進一步恐怖襲擊的計劃_「他已準備好從布魯塞爾重新展開恐襲_」雷恩德斯表示：「這也許是事實_因為我們在逮捕行動中發現大量武器_其中還有重型武器_另外我們也發現他在布魯塞爾有新的聯絡網_」曾考慮變更警戒層級比利時內閣在阿布岱斯蘭落網後_曾召開國家安全會議討論是否變更國家警戒層級_當時決定維持次高第3級_車站和歐盟相關機構保持軍警巡邏_22日連環爆炸案發生後_比利時宣布進入國家緊急狀態_布魯塞爾爆炸前_比利時和法國發現大約有30名共犯_參與去年11月13日造成130人死亡的巴黎血腥屠殺_至於直接參與巴黎恐攻者則為10人_但警方現透露_發現另一名新共犯直接參巴黎恐攻_查出新共犯積極追緝比利時警方表示_這名24歲的新嫌犯_叫做拉克哈維（Najim Laachraoui）_他曾使用假名卡亞爾_陪同阿布岱斯蘭於去年9月前往匈牙利_警方相信_拉克哈維在2013年2月曾進入敘利亞_警方在巴黎恐攻現場使用的槍枝和自殺攻擊爆炸物中_發現他的DNA_比、法警方目前正積極會商如何追緝這名新嫌犯_不過_比利時警方坦承_未能清楚掌握阿布岱斯蘭從去年11月14日到今年3月18日被捕_這段期間的行蹤全貌_英國安全事務專家認為_儘管比利時警方從阿布岱斯蘭供詞中獲悉IS將發動新攻擊_但IS的情報網絡顯然相當緊密_阿布岱斯蘭極可能根本不知道展開攻擊的確切時間和地點_(中國時報)\n",
      "布魯塞爾恐攻》機場彷彿煉獄 天花板崩垮 整棟樓在晃 焦點要聞 20160323 2170 比利時首都布魯塞爾22日上午將近8時（台灣時間下午3時）突然發生機場連環爆炸案_該爆炸發生在札范登（Zaventem）機場出境大廳的2個報到櫃台_造成14人死亡、81人受傷_機場的大片落地窗碎裂一地_天花板崩垮_民眾驚慌向外逃竄_滿身是血與灰塵的傷者倒臥在地_彷彿人間煉獄_現場影片顯示_浩劫過後的機場內部一片凌亂_崩塌的天花板與旅客遺落的行李箱四處散落_還有小範圍的火災發生_另外根據媒體公布的照片_受傷民眾衣服殘破_全身布滿灰塵_臉部及身上血跡斑斑_慘不忍睹_數十輛救護車停在機場外面_救難人員不停把傷者運上車送往醫院_機場工作人員與消防隊則留在建築物內將火勢撲滅_英國「天空新聞網」（Sky News）中東特派員洛西原本將從該地飛往以色列特拉維夫_他心有餘悸地說_爆炸當時「感覺整棟樓都在搖晃」_旅客維塞爾說_「我前去辦理登機手續時_聽到2聲爆炸聲…所有東西都掉下來…人們逃跑_很多人倒在地上_很多人受傷」_一名瑞士國際空港服務有限公司員工表示_他從戶外看見機場大樓冒煙_窗戶全部都被震碎_札范登機場一名女員工接受中央社記者訪問時說_傳出爆炸聲響後_出境大廳很多東西都被炸毀_碎片散落一地_由於事發地點是大家送機道別的地方_許多人受傷倒地_還有人喪命_該名員工很快被疏散到機場外_但仍驚魂未定_面露驚恐_她說_「爆炸一發生_大家馬上就知道是怎麼一回事_只見大家紛紛拔腿狂奔_爭相逃離現場_她還看到一名跟著大家一起逃離的女孩慌亂中跌倒在地_女孩驚嚇哭喊說_「是炸彈！」比利時媒體報導_機場2起爆炸案的間隔約15秒_有一起疑似是由一名自殺炸彈客所犯下_(中國時報)\n",
      "布魯塞爾恐攻》地鐵煙塵爆出 乘客驚慌外衝 全身是血 焦點要聞 20160323 973 布魯塞爾機場22日早晨發生連環爆炸後_早上9時11分_鄰近歐盟總部的梅爾比克（Maelbeek）地鐵站又發生爆炸_造成20人喪生、106人受傷_目擊者表示_當時正是繁忙的通勤時段_一聲巨響過後_許多乘客驚惶的逃出車站_有人渾身是血_場面一片混亂_該地鐵站位置鄰近歐盟總部與歐洲議會_爆炸發生後_布魯塞爾地鐵營運商立即宣布_全面關閉地鐵系統_市區電纜車及巴士也停駛_歐盟執委會則緊急通知所有員工待在室內_或留在家中不要出門_地鐵當局表示_當時一輛3節列車正準備駛離車站_往下一站Arts-Loi前進_位於中間車廂的炸彈忽然引爆_當時列車還沒有駛離月台_司機連忙緊急停車_並且疏散第一與第三節車廂內乘客_爆炸發生後_從當地電視畫面可見到_地鐵入口不斷冒出濃濃黑煙_許多醫護人員在地鐵站外對傷者進行急救_大批警察也趕到_封鎖現場_恰好住在該地鐵站附近的英國遊客海斯表示_地鐵站發生爆炸時_他正走過站前_「我看到很多人驚慌的從站內往外衝_他們身上都是血跡_有人受傷了_場面一片混亂_」他說_「有人叫我躲到室內_不要出來_有一度我聽見很多人大喊_還有人攙扶傷者_從地鐵站走到街道上_現在那個區域都被警方圍起來了」_爆炸發生後_一名當地人用手機記錄_所有乘客在光線微弱的地鐵甬道中沿著鐵軌行進_以進行疏散_另一名目擊者表示_事發當時他聽見「砰」的一聲巨響_也感受到爆炸的震波_接著看到很多煙霧與灰塵_從地鐵甬道裡冒出來_起先很多人在哭_大家都很驚慌_不知道發生了什麼事_雖然事發當時是繁忙的通勤時段_但當局很快的就將地鐵站內的人員淨空_(中國時報)\n",
      "布魯塞爾恐攻》英法如臨大敵 急開國安會議 焦點要聞 20160323 1278 比京布魯塞爾橫遭連環恐襲_與比利時接壤的法國、德國和荷蘭22日加強機場、公共運輸樞紐的保安與邊境控管_而法、英兩國如臨大敵_相繼召開緊急國安會議對應_連通法、比、德與荷蘭的「大力士高速列車」（Thalys）則全線停止營運_行經英倫海峽的高鐵「歐洲之星」（Eurostar）_往來布魯塞爾班車也告停駛_提供全球民航班機動態服務的網站「Flightradar24」估計_飛往布魯塞爾的國際航班改降或取消者逾200_法國總統奧朗德在愛麗榭宮主持最高層級的「國防委員會議」_並責成內政部增派1600名員警_在法比邊境關卡與法境海空以及鐵路基礎設施嚴密攔檢_內政部長卡澤納夫表示_法境大眾運輸系統僅限持有票證或身分證的民眾進入_旅客可能須接受搜身檢查_首都巴黎主要車站、地鐵22日中午起_可見員警四布_戴高樂國際機場8座航廈和維安部署也大幅增強_巴黎去年11月甫遭遇協同恐襲_無辜死者多達130人_奧朗德強調_歐陸各國必須團結一致力抗恐怖主義_巴黎市長伊達歌則宣布_22日晚將以代表比利時國旗的黑、黃、紅三色光打亮地標艾菲爾鐵塔_以示與比國人民同心同德_比利時北鄰荷蘭19日提高境內機場的安全警戒_同時加強荷比邊境控管_比國東鄰、歐盟龍頭德國最繁忙的法蘭克福國際機場_19日也提高戒備_在與歐陸隔著英倫海峽的英國_首相卡麥隆主持由內政、外交、國防、情報等多部會參與的內閣「COBRA」會議_啟動緊急應變機制_倫敦希斯洛與蓋特威克國際機場加強入出境旅客安檢_荷槍實彈的員警四處巡邏_英國警方日前表示_倫敦可能同時發生近10起大規模恐襲攻_呼籲民眾嚴防_卡麥隆對布魯塞爾遇襲甚感「震驚與關切」_強調英方將竭盡所能協助比利時政府_(中國時報)\n",
      "布魯塞爾恐攻》古樸城市變聖戰士溫床 4年來500人赴敘利亞 焦點要聞 20160323 3091 比利時首都布魯塞爾是一個古樸而繁華的城市_以巧克力和啤酒聞名_但如今也變成聖戰士招兵買馬的沃土_巴黎恐怖攻擊案唯一倖存的嫌犯阿布岱斯蘭_最近就是在布魯塞爾莫倫比克區落網_比利時當局坦承_恐怖組織「伊斯蘭國」（IS）仍繼續在比國招兵買馬_依人口比例計算_西歐國家中_比利時人赴敘利亞參戰的人數最多_專家說_自2012年以來_近500名男女離開比利時_前往敘利亞和伊拉克_同一時期_超過100名比利時人從IS地盤返國_許多人立即被捕_但沒有人知道有多少漏網之魚_比利時官方阻止不了當地民眾被「伊斯蘭國」招募前往IS占領區_也許更擔心這些聖戰士會回到歐洲_發動巴黎恐攻式的攻擊行動_美國有線電視新聞網（CNN）近幾周走訪布魯塞爾穆斯林移民聚集、被視為激進聖戰士意識型態溫床的莫倫比克區（Molenbeek）_一名化名為阿里的受訪者說_他的兩個兄弟是比利時的伊斯蘭激進組織成員_兩人前往敘利亞後一人戰死_另一人回國後遭監禁_阿里認為_歧視和「缺乏機會」_導致很多年輕人認為自己無法被社會接受_這種被比利時社會邊緣化的感受_正好被伊斯蘭極端組織的利用_加入聖戰士行列_他說：「比利時排斥外來的孩子和年輕人_他們會說『他們是外來者_為什麼我們應該給他們工作？』他們仇視我們_他們說我們沒用_因此當聖戰士遊說這些年輕人前往敘利亞時_年輕人會想『好啊_我們到那裡_會是有用的人』_」阿里認為_比利時當局為了擺脫這些年輕人_於是睜一隻眼閉一隻眼_讓他們離開_但布魯塞爾的「去激進化中心」創辦人戴米說_不能把他們關起來_不准他們離開_不過可以給他們一個未來_幫助他們了解自己的價值_他們就不會想走_(中國時報)\n",
      "布魯塞爾恐攻》只差30秒 台灣客驚魂：差點逃不出來 焦點要聞 20160323 4431 與死神擦身而過！26歲的劉筱沁在今年1月底到比利時打工度假_昨日與姊姊在當地時間、上午9時55分於布魯塞爾機場櫃檯辦理離境_準備前往羅馬度假_突遇爆炸案_劉筱沁接受本報訪問時_驚魂未定地表示_「只差30秒_我差點就逃不出來_」劉筱沁表示_爆炸發生時她與姊姊兩人離爆炸櫃檯不到500公尺_突然一聲巨響_兩人被爆炸威力震倒在地_幸虧兩人即時逃出_「30秒後天花板就砸下來了_」家住桃園的劉筱沁說_她原本與姊姊欲搭乘昨日上午飛往羅馬的班機_兩人在瑞安航空10號櫃台報到時_距離不遠的5號櫃檯處突然爆炸_「我親眼看到櫃檯冒火_服務人員被炸飛出櫃檯_」由於爆炸聲響劇烈_劉筱沁第一反應認為遭到恐怖攻擊_立即跟著地勤人員到機場辦公室避難_她也特別感謝航空人員第一時間將民眾疏散至辦公室內_安撫民眾情緒_在辦公室避難30分鐘後_姊妹倆回到爆炸發生的出境大廳_警方及軍方人員已趕到_現場斷垣殘壁_瀰漫粉塵及煙硝味_劉筱沁回憶_現場1人被天花板壓住_5、6人傷勢嚴重_還有一具遺體已蓋上白布_劉筱沁與姊姊目前平安並無傷勢_以也回報外交部駐比利時辦事處_並在上午11時左右搭上比利時官方加派的疏散巴士前往魯汶_(中國時報)\n",
      "布魯塞爾恐攻》我外館1人輕傷 僑民沒事 焦點要聞 20160323 562 我國駐歐盟代表處一名館員在22日布魯塞爾地鐵站爆炸案中_遭爆裂火花波及_受到輕傷與驚嚇_但無大礙_這名館員抵達辦公室後_隨即在同仁陪同下返家休息_我國駐歐盟兼駐比利時大使董國猷表示_在布魯塞爾發生連環爆炸案中_台灣僑民和學生均安_代表處距離爆炸案發生地點之一的布魯塞爾梅爾比克地鐵站大約只有800到1000公尺左右_區域內除歐盟理事會和其他歐盟相關機構外_美國和法國大使館也在附近_董國猷表示_22日當地時間上午9:15分他在歐洲議會有約_前往拜會時_當時布魯塞爾機場雖已傳出爆炸事件_但市區道路交通在巔峰時間_依然擁擠_未見任何異狀_不過_歐盟公務和使館聚集的Shuman區梅爾比克地鐵站隨後發生爆炸_行人和車輛疏散_鄰近道路封鎖_從代表處辦大樓窗口望出_街上幾無行人_董國猷說_爆炸案發生後_布魯塞爾居民不免緊張_但整體氣氛還算平和_「可是_心中恐懼總是存在的_」(中國時報)\n",
      "布魯塞爾恐攻》防不勝防 觀光航空業重傷 焦點要聞 20160323 1382 布魯塞爾周二上午驚傳機場和地鐵爆炸_讓飽受恐怖攻擊威脅的觀光和航空產業再次遭重創_消息一出_航空和觀光類股應聲大跌_巴黎繼去年11月和今年1月接連發生恐攻後_布魯塞爾爆炸案緊接在後_Raymond James飯店和觀光業分析師李查（Julien Richer）表示_「我們從未遭遇這種挑戰_這不只是一次性事件_而是連續事件_可能要花一點時間才能復原_」由於投資人擔心爆炸將影響航空公司訂位_使得周二歐洲航空類股大跌_法航損失7600萬美元去年11月巴黎恐攻重創多家歐洲航空公司_包括最大航空公司法航－荷蘭皇家航空（KLM）、英國航空（BA）母公司IAG_和歐洲最大廉價航空瑞安航空（Ryanair）等_單是法航便因為巴黎恐攻而損失7,600萬美元的營收_法航－荷蘭皇家航空周二法股午盤重摔5％_報8.01歐元_雅高飯店集團（Accor）跌逾5％_每股36.98歐元_在布魯塞爾有4家飯店的喜達屋飯店集團股價在紐約盤中則跌1.8％_每股82.72美元；希爾頓集團也跌1.8％_IAG在倫敦股市午盤下跌2.4％_報544.5便士_德國漢莎航空盤中跌2.9％_瑞安航空跌2.3％_歐洲機場紛加強安檢布魯塞爾機場在爆炸後隨即取消航班_宣布關閉機場_機場官員表示_機場至少關閉至當地時間周三上午6點_歐洲機場也紛紛加強安檢_德國最大機場法蘭克福機場發言人表示_已提高安全措施_英國政府也在爆炸發生後召開緊急內閣會議_歐洲最繁忙機場－倫敦希斯洛機場表示已加強員警部署_英國券商CMC Markets分析師羅勒說_布魯塞爾機場爆炸對航空業無疑是壞消息_去年底連串恐攻讓航空業大受打擊_近期好不容易旅客才回籠_卻又發生新攻擊事件_官方可能加強安全控管_此舉將讓旅客遊興大減_(中國時報)\n",
      "布魯塞爾恐攻》好險！早半小時到班 合庫駐比行員 躲過一劫 焦點要聞 20160323 2190 千鈞一髮！合庫子行比利時台灣聯合銀行駐地行員表示_行員們在爆炸案發生前半小時都已經來到辦公室上班_因為爆炸案發生的其中一處Maelbeek地鐵站_正好是該行行員每天上班必經之地_也是辦公室附近主要地鐵站_而且距離該行只有約600公尺_走路只要6分鐘_非常驚險_比利時台灣聯合銀行的辦公室位於布魯塞爾的市中心大街Boulevard du Regent一座大樓的7樓_記者是在當地時間上午11點訪問比利時聯合銀行當地的人員_據當地人員轉述_由於9點發生爆炸案_窗外不時可聽見警車、救護車不斷呼嘯而過的聲音_讓行員非常意外_直呼「下班後會快回家_不能在外面」_比利時台灣聯合銀行董事長目前由合庫總經理林鴻琛兼任_多年以來_合庫銀一直由總座兼任該比利時子行的董事長_接下來的股東會_即將在4月中旬召開_金融圈人士指出_由於比利時聯合銀行_國內八大公股行庫_以及部分民營銀行等都有入股_因此_這個現在被「恐攻」高度聚焦的城市_各大行庫的相關主管即使再不願意_恐怕還是得硬著頭皮飛來比利時開股東會_歐洲接二連三發生恐攻事件_先是巴黎_現在則是被視為是恐怖分子大本營的布魯塞爾_比利時聯合銀行副理蔡友人指出_爆炸案約在當地時間9點發生_但因為行員都在8點半到8點50分間陸續抵達辦公室_行員都平安_(中國時報)\n"
     ]
    }
   ],
   "source": [
    "for z in range(0,len(d)):\n",
    "    print d[z]['title'],d[z]['sort'],d[z]['time'],d[z]['people'],d[z][\"content\"]  #查詢KEY\n",
    "    #print d[z]['all']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4.焦點新聞 \"總分頁\" 結算  \"分頁\" (當日) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.chinatimes.com/history-by-date/2016-03-23-2601?page=0\n",
      "http://www.chinatimes.com/history-by-date/2016-03-23-2601?page=1\n",
      "http://www.chinatimes.com/history-by-date/2016-03-23-2601?page=2\n",
      "http://www.chinatimes.com/history-by-date/2016-03-23-2601?page=3\n",
      "http://www.chinatimes.com/history-by-date/2016-03-23-2601?page=4\n",
      "http://www.chinatimes.com/history-by-date/2016-03-23-2601?page=5\n",
      "http://www.chinatimes.com/history-by-date/2016-03-23-2601?page=6\n",
      "http://www.chinatimes.com/history-by-date/2016-03-23-2601?page=7\n",
      "http://www.chinatimes.com/history-by-date/2016-03-23-2601?page=8\n",
      "http://www.chinatimes.com/history-by-date/2016-03-23-2601?page=9\n",
      "http://www.chinatimes.com/history-by-date/2016-03-23-2601?page=10\n",
      "http://www.chinatimes.com/history-by-date/2016-03-23-2601?page=11\n",
      "http://www.chinatimes.com/history-by-date/2016-03-23-2601?page=12\n",
      "http://www.chinatimes.com/history-by-date/2016-03-23-2601?page=13\n",
      "http://www.chinatimes.com/history-by-date/2016-03-23-2601?page=14\n",
      "http://www.chinatimes.com/history-by-date/2016-03-23-2601?page=15\n",
      "http://www.chinatimes.com/history-by-date/2016-03-23-2601?page=16\n",
      "http://www.chinatimes.com/history-by-date/2016-03-23-2601?page=17\n",
      "http://www.chinatimes.com/history-by-date/2016-03-23-2601?page=18\n",
      "http://www.chinatimes.com/history-by-date/2016-03-23-2601?page=19\n",
      "http://www.chinatimes.com/history-by-date/2016-03-23-2601?page=20\n"
     ]
    }
   ],
   "source": [
    "import re  #正規化\n",
    "import math\n",
    "total_pages1=soup.select('.listLeft ul')[0].select('li')[0].select('span')[1]\n",
    "total_pages2=(str(total_pages1.text))\n",
    "\n",
    "dic=[total_pages2,\"apple\"]\n",
    "\n",
    "for ele in dic: \n",
    "    #print dic\n",
    "    m=re.search('\\((?P<username>\\d+)\\)',ele)\n",
    "    if m:\n",
    "        #print type(m.group('username'))\n",
    "        total_pages=int(m.group('username'))   #總頁數\n",
    "#print total_pages\n",
    "#print how_many_new_in_page\n",
    "how_many_page=int(math.ceil(total_pages/float(how_many_new_in_page))) #有多少分頁\n",
    "#print how_many_page\n",
    "\n",
    "for j in xrange(how_many_page+1):\n",
    "    secondPages='http://www.chinatimes.com/history-by-date/2016-03-23-2601?page={}'.format(j)  #僅此2016-03-19-2601，之後要做不同天的!\n",
    "    print secondPages\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.分類+存檔案(在本機端分類新聞類別的資料夾)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.total pages    試著找 2013-2016 總篇數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-116-087dbb5721b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mres_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0msoup_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mtotal_pages1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msoup_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.listLeft ul'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'li'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'span'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0mtotal_pages2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_pages1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mdic\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtotal_pages2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"apple\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#製造日期的迴圈\n",
    "import re  #正規化\n",
    "import math\n",
    "import datetime as dt\n",
    "import time \n",
    "startdate = dt.datetime(2016,3,22)\n",
    "endate = dt.datetime(2016,3,23)\n",
    "totaldays = (endate - startdate).days + 1 #期間有幾天\n",
    "count=0\n",
    "k=0\n",
    "for daynumber in range(totaldays):\n",
    "    datestring = (startdate + dt.timedelta(days = daynumber)).date().strftime(\"%Y-%m-%d\") \n",
    "    #print datestring\n",
    "    #每一天的蘋果新聞頁面\n",
    "    #http://www.chinatimes.com/history-by-date/2016-03-19-2601?page=1 \n",
    "    page_url = 'http://www.chinatimes.com/history-by-date/{}-2601?page=1'.format(datestring)\n",
    "    #print page_url\n",
    "    res_test=requests.get(page_url)\n",
    "    soup_test=BS(res_test.text)\n",
    "    total_pages1=soup_test.select('.listLeft ul')[0].select('li')[0].select('span')[1]\n",
    "    total_pages2=(str(total_pages1.text))\n",
    "    dic=[total_pages2,\"apple\"]\n",
    "    for ele in dic: \n",
    "        #print dic\n",
    "        m=re.search('\\((?P<username>\\d+)\\)',ele)\n",
    "        if m:\n",
    "            #print type(m.group('username'))\n",
    "            total_pages=int(m.group('username'))   #一天新聞中的總頁數\n",
    "            count+=total_pages                     #多天新聞的頁數累加\n",
    "            print count\n",
    "            #print k\n",
    "            #k=k+1                                ##2013-1-1~2013-12-31共有 94790 篇新聞 (共 分鐘抓完)\n",
    "            time.sleep(0.6)                      #2015-1-1~2016-3-22共有 101002 篇新聞 (共25分鐘抓完)\n",
    "print count                                      #2014-1-1~2014-12-31共有 89780 篇新聞 (共15分鐘抓完)\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1.焦點新聞 \"總分頁\" 結算  \"分頁\" (多日) + (1+2+3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-03-17\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-b55cb1f9295f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mres_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0msoup_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mtotal_pages1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msoup_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.listLeft ul'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'li'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'span'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[1;31m#print total_pages1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mtotal_pages2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_pages1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import re  #正規化\n",
    "import math\n",
    "import datetime as dt\n",
    "import time \n",
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import json\n",
    "import ast  #轉換成json需要套件\n",
    "startdate = dt.datetime(2016,3,17)\n",
    "endate = dt.datetime(2016,3,23)\n",
    "totaldays = (endate - startdate).days + 1 #期間有幾天\n",
    "count=0\n",
    "k=0\n",
    "all_json=''\n",
    "for daynumber in range(totaldays):\n",
    "    datestring = (startdate + dt.timedelta(days = daynumber)).date().strftime(\"%Y-%m-%d\") \n",
    "    print datestring\n",
    "    #每一天的蘋果新聞頁面\n",
    "    #http://www.chinatimes.com/history-by-date/2016-03-19-2601?page=1 \n",
    "    page_url = 'http://www.chinatimes.com/history-by-date/{}-2601?page=1'.format(datestring)\n",
    "    #print page_url\n",
    "    res_test=requests.get(page_url)\n",
    "    soup_test=BS(res_test.text)\n",
    "    total_pages1=soup_test.select('.listLeft ul')[0].select('li')[0].select('span')[1]\n",
    "    #print total_pages1\n",
    "    total_pages2=(str(total_pages1.text))\n",
    "    lis=soup_test.select('.listRight li')\n",
    "    how_many_new_in_page=len(lis)\n",
    "    dic=[total_pages2,\"apple\"]\n",
    "    for ele in dic: \n",
    "        #print dic\n",
    "        m=re.search('\\((?P<username>\\d+)\\)',ele)\n",
    "        if m:\n",
    "            total_pages=int(m.group('username'))   #總頁數\n",
    "            #print total_pages\n",
    "        \n",
    "    how_many_page=int(math.ceil(total_pages/float(how_many_new_in_page))) #有多少分頁\n",
    "    #print how_many_page\n",
    "    \n",
    "    #  以上得知 ↑↑↑   ------→ 每一天的新聞，各自有多少篇數，還有多少分頁 ---------------------------\n",
    "    \n",
    "    for j in xrange(how_many_page+1):\n",
    "        secondPages='http://www.chinatimes.com/history-by-date/{}-2601?page={}'.format(datestring,j)  #僅此2016-03-19-2601，之後要做不同天的!\n",
    "        print secondPages  \n",
    "    \n",
    "    #  以上得知 ↑↑↑   ------→ 每一天的新聞不同的分頁數 ---------------------------\n",
    "        url=secondPages\n",
    "        res=requests.get(url)\n",
    "        soup=BS(res.text)\n",
    "        #print soup\n",
    "        mainPage='http://www.chinatimes.com/'\n",
    "        lis=soup.select('.listRight li')\n",
    "        how_many_new_in_page=len(lis)\n",
    "        l=0\n",
    "        #print how_many_new_in_page\n",
    "        \n",
    "        for li in lis:\n",
    "            print l\n",
    "            l=l+1\n",
    "            if l<12:   #單頁報導要寫入幾天 \n",
    "                title=li.select('h2')[0].text.strip() #list→unicode  加了[0]→'bs4.element.Tag' 可顯示中文字\n",
    "                #print title  #2016/3/23 ok\n",
    "                category=li.select('.kindOf a')[0].text.strip() #類別\n",
    "                #print category # 2016/3/23 ok\n",
    "                Data2=soup.select('.page_index li')[1]\n",
    "                Data1=Data2.select('h6 a')[0]['href'][0:].split('-')\n",
    "                Data=Data1[0]+Data1[1]+Data1[2]              #yyyymmdd  日期    \n",
    "                #print Data   # 2016/3/23 ok\n",
    "                newspaper_office='中國時報'                  #報社\n",
    "                page1=li.select('h2 a')[0]['href'][0:]      #新聞連結，進入到內文的階段\n",
    "                each_new_page_url=mainPage+page1 \n",
    "                #print each_new_page_url  # 2016/3/23 ok\n",
    "                second_url=requests.get(each_new_page_url)\n",
    "                second_usoup=BS(second_url.text)  \n",
    "                hits=second_usoup.findAll('div',{'class':'art_click clear-fix'})[0]\n",
    "                number_of_hits = int(hits.select('span')[1].text)            #點擊率\n",
    "                all_p_tags=second_usoup.select('.clear-fix p')[:-6]\n",
    "                content_list=[]\n",
    "                for p_tag in all_p_tags:\n",
    "                    p_tag1=str(p_tag).replace(\"，\",\"_\")\n",
    "                    p_tag2=str(p_tag1).replace(\"。\",\"_\")\n",
    "                    p_tag3=str(p_tag2).replace(\"<p>\",\"\")\n",
    "                    p_tag4=str(p_tag3).replace(\"</p>\",\"\")\n",
    "                    #print p_tag4    #2016/3/23 ok                         #內文(已將 \"，\" & \"。\"轉換)\n",
    "                    content_list.append(p_tag4)                            #將全部 標籤 P寫入 list中\n",
    "                separator=''\n",
    "                joined_content=separator.join(content_list)              #將 list [i] 以 \"\" 合併，得到全文內容    |獨家秘技!!!!| \n",
    "                #print joined_content  #2016/3/23 ok\n",
    "                keyword=second_usoup.select('.a_k a')\n",
    "                i=0\n",
    "                kwords_list=[]\n",
    "                for kwords in range(len(keyword)):\n",
    "                    keyword=second_usoup.select('.a_k a')[i]\n",
    "                    #print keyword.text                                       #此頁新聞的關鍵字\n",
    "                    i=i+1\n",
    "                    kwords_list.append(keyword.text)\n",
    "                separator='_'    \n",
    "                joined_kwords=separator.join(kwords_list)               #將 list [i] 以 \"\" 合併，得到關鍵字全部內容 \n",
    "                #print joined_kwords    #2016/3/23 ok\n",
    "\n",
    "                m = {\n",
    "                     'url':each_new_page_url,'title':title,'sort':category,\\\n",
    "                     'content':joined_content,'time':Data,'people':number_of_hits,\\\n",
    "                     'keywords':joined_kwords,'newspaper_office':'中國時報'\n",
    "                     }                          #填入json格式dict\n",
    "\n",
    "                all_json+=str(m)+','            #dict不可以先加 所以先變STR相加   \n",
    "                time.sleep(0.6) \n",
    "            else:\n",
    "                break\n",
    "\n",
    "#2016/3/23 ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55年敵對成往事 破冰古巴 歐巴馬來了 焦點要聞 20160322 3189\n",
      "陸甘復交有續集？我22邦交國 21國亮綠燈 焦點要聞 20160322 37152\n",
      "新華網解讀習談話 回應蔡英文「增強兩岸命運共同體認知」 焦點要聞 20160322 25916\n",
      "工業總會：籲蔡英文向在野黨及兩岸遞橄欖枝 蔡英文：會確保國家穩... 焦點要聞 20160322 1767\n",
      "與蘇起走太近？ 洪奇昌遭新系除名 焦點要聞 20160322 15213\n",
      "民共缺乏互信 兩岸關係恐顛簸 焦點要聞 20160322 3589\n",
      "陸甘復交隔天 國台辦拋「熱線」 我拒絕 焦點要聞 20160322 3645\n",
      "美挺我入國際刑警組織 陸反對 焦點要聞 20160322 1428\n",
      "拿美金敲開古巴市場？有得拚 焦點要聞 20160322 960\n",
      "中、古關係躍進 美國急了 焦點要聞 20160322 3311\n",
      "與大聯盟簽約可望合法化 古巴球員不必再冒死追夢 焦點要聞 20160322 1034\n",
      "55年敵對成往事 破冰古巴 歐巴馬來了 焦點要聞 20160322 3189\n",
      "陸甘復交有續集？我22邦交國 21國亮綠燈 焦點要聞 20160322 37152\n",
      "新華網解讀習談話 回應蔡英文「增強兩岸命運共同體認知」 焦點要聞 20160322 25917\n",
      "工業總會：籲蔡英文向在野黨及兩岸遞橄欖枝 蔡英文：會確保國家穩... 焦點要聞 20160322 1767\n",
      "與蘇起走太近？ 洪奇昌遭新系除名 焦點要聞 20160322 15213\n"
     ]
    }
   ],
   "source": [
    "a=eval(all_json)                       #AST套件 轉換 all_json 成 json\n",
    "d1 =  json.dumps(a)\n",
    "d =  json.loads(d1)\n",
    "for z in range(0,len(d)):\n",
    "    print d[z]['title'],d[z]['sort'],d[z]['time'],d[z]['people']  #查詢KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#要記得紀錄跑到哪裡  記錄下來下次可以繼續跑"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
