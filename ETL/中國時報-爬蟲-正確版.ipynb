{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4 30\n",
      "2013-04-01\n",
      "1 2 3 4 5 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=0 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000363-260102\n",
      "6 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=0 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000366-260102\n",
      "7 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=0 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000368-260102\n",
      "8 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=0 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000369-260102\n",
      "9 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=0 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000370-260102\n",
      "10 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=0 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000371-260102\n",
      "11 12 13 14 15 16 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=1 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000363-260102\n",
      "17 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=1 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000366-260102\n",
      "18 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=1 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000368-260102\n",
      "19 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=1 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000369-260102\n",
      "20 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=1 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000370-260102\n",
      "21 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=1 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000371-260102\n",
      "22 23 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=2 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000376-260102\n",
      "24 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=2 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000377-260102\n",
      "25 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=2 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000378-260102\n",
      "26 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=2 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000380-260102\n",
      "27 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=2 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000381-260102\n",
      "28 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=2 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000385-260102\n",
      "29 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=2 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000394-260102\n",
      "30 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=2 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000396-260102\n",
      "31 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=2 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000411-260102\n",
      "32 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=2 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000422-260102\n",
      "33 34 35 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=3 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000439-260102\n",
      "36 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=3 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000440-260102\n",
      "37 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=3 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000444-260102\n",
      "38 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=3 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000445-260102\n",
      "39 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=3 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000447-260102\n",
      "40 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=3 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000450-260102\n",
      "41 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=3 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000451-260102\n",
      "42 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=3 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000454-260102\n",
      "43 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=3 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000456-260102\n",
      "44 45 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=4 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000463-260102\n",
      "46 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=4 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000468-260102\n",
      "47 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=4 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000473-260102\n",
      "48 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=4 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000475-260102\n",
      "49 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=4 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000477-260102\n",
      "50 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=4 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000479-260102\n",
      "51 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=4 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000480-260102\n",
      "52 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=4 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000483-260102\n",
      "53 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=4 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000485-260102\n",
      "54 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=4 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000488-260102\n",
      "55 56 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=5 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000497-260102\n",
      "57 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=5 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000502-260102\n",
      "58 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=5 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000529-260102\n",
      "59 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=5 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000535-260102\n",
      "60 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=5 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000537-260102\n",
      "61 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=5 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000552-260102\n",
      "62 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=5 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000562-260102\n",
      "63 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=5 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000563-260102\n",
      "64 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=5 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000565-260102\n",
      "65 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=5 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000572-260102\n",
      "66 67 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=6 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000579-260102\n",
      "68 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=6 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000588-260102\n",
      "69 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=6 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000590-260102\n",
      "70 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=6 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000592-260102\n",
      "71 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=6 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000594-260102\n",
      "72 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=6 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000595-260102\n",
      "73 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=6 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000601-260102\n",
      "74 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=6 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000603-260102\n",
      "75 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=6 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000611-260102\n",
      "76 第幾分頁→ http://www.chinatimes.com/history-by-date/2013-04-01-2601?page=6 這一篇報導→ http://www.chinatimes.com//newspapers/20130401000614-260102\n",
      "77"
     ]
    }
   ],
   "source": [
    "import re  #正規化\n",
    "import math\n",
    "import datetime as dt\n",
    "import time \n",
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import json\n",
    "import ast  #轉換成json需要套件\n",
    "how_much_crawl=1\n",
    "year_2013={1: 31, 2: 28, 3: 31, 4: 30, 5: 31, 6: 30, 7: 31, 8: 31, 9: 30, 10: 31, 11: 30, 12: 31}\n",
    "#year_2014={ 1: 31, 2: 28, 3: 31, 4: 30, 5: 31, 6: 30, 7: 31, 8: 31, 9: 30, 10: 31, 11: 30, 12: 31}\n",
    "#year_2015={ 7: 31, 8: 31, 9: 30, 10: 31, 11: 30, 12: 31}\n",
    "#year_2016={1: 31, 2: 29, 3: 31, 4: 30, 5: 31, 6: 30, 7: 31, 8: 31, 9: 30, 10: 31, 11: 30, 12: 31}\n",
    "\n",
    "tic = time.clock()\n",
    "failure_url_2013=[]\n",
    "for mm in range(4,7):\n",
    "    dd=year_2013[mm]\n",
    "    print mm ,dd\n",
    "    startdate = dt.datetime(2013,mm,1)\n",
    "    endate = dt.datetime(2013,mm,dd)\n",
    "    totaldays = (endate - startdate).days + 1 #期間有幾天\n",
    "    count=0\n",
    "    k=0\n",
    "    all_json=[]\n",
    "    \n",
    "    for daynumber in range(totaldays):\n",
    "        datestring = (startdate + dt.timedelta(days = daynumber)).date().strftime(\"%Y-%m-%d\") \n",
    "        print datestring\n",
    "        #每一天的蘋果新聞頁面\n",
    "        #http://www.chinatimes.com/history-by-date/2016-03-19-2601?page=1 \n",
    "        page_url = 'http://www.chinatimes.com/history-by-date/{}-2601?page=1'.format(datestring)\n",
    "        #print page_url\n",
    "        res_test=requests.get(page_url)\n",
    "        soup_test=BS(res_test.text)\n",
    "        total_pages1=soup_test.select('.listLeft ul')[0].select('li')[0].select('span')[1]\n",
    "        #print total_pages1\n",
    "        total_pages2=(str(total_pages1.text))\n",
    "        lis=soup_test.select('.listRight li')\n",
    "        how_many_new_in_page=len(lis)\n",
    "        dic=[total_pages2,\"apple\"]\n",
    "        for ele in dic: \n",
    "            #print dic\n",
    "            m=re.search('\\((?P<username>\\d+)\\)',ele)\n",
    "            if m:\n",
    "                total_pages=int(m.group('username'))   #總頁數\n",
    "                #print total_pages\n",
    "\n",
    "        how_many_page=int(math.ceil(total_pages/float(how_many_new_in_page))) #有多少分頁\n",
    "        #print how_many_page\n",
    "\n",
    "        #  以上得知 ↑↑↑   ------→ 每一天的新聞，各自有多少篇數，還有多少分頁 ---------------------------\n",
    "\n",
    "        for j in xrange(how_many_page+1):#how_many_page+1\n",
    "            secondPages='http://www.chinatimes.com/history-by-date/{}-2601?page={}'.format(datestring,j)  #僅此2016-03-19-2601，之後要做不同天的!\n",
    "            #print \"\\n\",secondPages  \n",
    "            time.sleep(1.5)    #要睡喔! 不然的話會失敗，網頁沒開完整 就開始抓就會容易失敗!!!!!!!!!!!!!!!!!! Q_Q\n",
    "        #  以上得知 ↑↑↑   ------→ 每一天的新聞不同的分頁數 ---------------------------\n",
    "            url=secondPages\n",
    "            res=requests.get(url)\n",
    "            soup=BS(res.text)\n",
    "\n",
    "            #print soup\n",
    "            mainPage='http://www.chinatimes.com/'\n",
    "            lis=soup.select('.listRight li')\n",
    "            how_many_new_in_page=len(lis)\n",
    "            l=0\n",
    "            #print how_many_new_in_page\n",
    "            for li in lis:\n",
    "                #print l,\"總頁數\", how_much_crawl,\"|\",\n",
    "                print how_much_crawl,\n",
    "                how_much_crawl=how_much_crawl+1\n",
    "                l=l+1\n",
    "                if l<len(lis):   #單頁報導要寫入幾天 #len(lis)\n",
    "                    title=li.select('h2')[0].text.strip() #list→unicode  加了[0]→'bs4.element.Tag' 可顯示中文字\n",
    "                    #print title  #2016/3/23 ok\n",
    "                    category=li.select('.kindOf a')[0].text.strip() #類別\n",
    "                    #print category # 2016/3/23 ok\n",
    "                    Data2=soup.select('.page_index li')[1]\n",
    "                    Data1=Data2.select('h6 a')[0]['href'][0:].split('-')\n",
    "                    Data=Data1[0]+Data1[1]+Data1[2]              #yyyymmdd  日期    \n",
    "                    #print Data   # 2016/3/23 ok\n",
    "                    newspaper_office='中國時報'                  #報社\n",
    "                    page1=li.select('h2 a')[0]['href'][0:]      #新聞連結，進入到內文的階段\n",
    "                    each_new_page_url=mainPage+page1 \n",
    "                    #print each_new_page_url  # 2016/3/23 ok\n",
    "                    second_url=requests.get(each_new_page_url)\n",
    "                    second_usoup=BS(second_url.text)\n",
    "                    #print len(second_usoup.findAll('div',{'class':'art_click clear-fix'})[0])\n",
    "                    try:\n",
    "                        title=second_usoup.select('header h1')[1]\n",
    "                        #print title.text                    #title\n",
    "                        if len(second_usoup.findAll('div',{'class':'art_click clear-fix'})[0])>1:\n",
    "\n",
    "                            hits=second_usoup.findAll('div',{'class':'art_click clear-fix'})[0]\n",
    "                            number_of_hits = int(hits.select('span')[1].text)            #點擊率\n",
    "                            \n",
    "                        else:\n",
    "                            number_of_hits=0\n",
    "                            failure_url_2013.append(each_new_page_url)\n",
    "                            print \"第幾分頁→\", secondPages, \"這一篇報導→\",each_new_page_url\n",
    "                    except IndexError:\n",
    "                        print \"沒有此網頁→\" ,each_new_page_url                   #title\n",
    "                        failure_url_2013.append(each_new_page_url)\n",
    "                        continue    \n",
    "\n",
    "                    all_p_tags=second_usoup.select('.clear-fix p')[:-6]\n",
    "                    content_list=[]\n",
    "                    for p_tag in all_p_tags:\n",
    "                        p_tag1=str(p_tag).replace(\"，\",\"_\")\n",
    "                        p_tag2=str(p_tag1).replace(\"。\",\"_\")\n",
    "                        p_tag3=str(p_tag2).replace(\"<p>\",\"\")\n",
    "                        p_tag4=str(p_tag3).replace(\"</p>\",\"\")\n",
    "                        #print p_tag4    #2016/3/23 ok                         #內文(已將 \"，\" & \"。\"轉換)\n",
    "                        content_list.append(p_tag4)                            #將全部 標籤 P寫入 list中\n",
    "                    separator=''\n",
    "                    joined_content=separator.join(content_list)              #將 list [i] 以 \"\" 合併，得到全文內容    |獨家秘技!!!!| \n",
    "                    #print joined_content  #2016/3/23 ok\n",
    "                    keyword=second_usoup.select('.a_k a')\n",
    "                    i=0\n",
    "                    kwords_list=[]\n",
    "                    for kwords in range(len(keyword)):\n",
    "                        keyword=second_usoup.select('.a_k a')[i]\n",
    "                        #print keyword.text                                       #此頁新聞的關鍵字\n",
    "                        i=i+1\n",
    "                        kwords_list.append(keyword.text)\n",
    "                    separator='_'    \n",
    "                    joined_kwords=separator.join(kwords_list)               #將 list [i] 以 \"\" 合併，得到關鍵字全部內容 \n",
    "                    #print joined_kwords    #2016/3/23 ok\n",
    "                    \n",
    "\n",
    "                    m = {\n",
    "                         'url':each_new_page_url,'title':title.text,'category':category,\\\n",
    "                         'content':joined_content,'date':Data,'hitcount':number_of_hits,\\\n",
    "                         'keyw':joined_kwords,'comp':'ChinaTimes'\n",
    "                         }                          #填入json格式dict\n",
    "\n",
    "                    #print m\n",
    "                    all_json.append(m)            #dict不可以先加 所以先變STR相加   \n",
    "                    time.sleep(1) \n",
    "                else:                           \n",
    "                    break               \n",
    "\n",
    "\n",
    "\n",
    "    toc = time.clock()\n",
    "    print toc-tic\n",
    "    #print m\n",
    "\n",
    " \n",
    "    d1=json.dumps(all_json)\n",
    "    #d=json.loads(d1)\n",
    "    #with open('C:/Users/BIG DATA/pythonETL/project/news/20130{}.json'.format(mm), 'w') as f:\n",
    "    with open('C:/Users/BIG DATA/pythonETL/project/news/2013{}.json'.format(mm), 'w') as f:\n",
    "        f.write(d1) \n",
    "    test1=str(failure_url_2013)    \n",
    "    with open('C:/Users/BIG DATA/pythonETL/project/news/failure_url_2013.txt', 'w') as f:\n",
    "        f.write(test1)     \n",
    "    #會有兩種 output!!!!!!   failure_news_2013.txt   &   20150{}.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print all_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "<h1>\\u9678\\u88fd\\u9020\\u696dPMI\\u90234\\u5347</h1> is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-66e79288247d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0md1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_json\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/BIG DATA/pythonETL/project/news/2015{}.json'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\BIG DATA\\Anaconda2\\lib\\json\\__init__.pyc\u001b[0m in \u001b[0;36mdumps\u001b[1;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, encoding, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    242\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mindent\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mseparators\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m         encoding == 'utf-8' and default is None and not sort_keys and not kw):\n\u001b[1;32m--> 244\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\BIG DATA\\Anaconda2\\lib\\json\\encoder.pyc\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[1;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[1;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m         \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\BIG DATA\\Anaconda2\\lib\\json\\encoder.pyc\u001b[0m in \u001b[0;36miterencode\u001b[1;34m(self, o, _one_shot)\u001b[0m\n\u001b[0;32m    268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[1;32m--> 270\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[1;32mC:\\Users\\BIG DATA\\Anaconda2\\lib\\json\\encoder.pyc\u001b[0m in \u001b[0;36mdefault\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m         \"\"\"\n\u001b[1;32m--> 184\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" is not JSON serializable\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: <h1>\\u9678\\u88fd\\u9020\\u696dPMI\\u90234\\u5347</h1> is not JSON serializable"
     ]
    }
   ],
   "source": [
    "import json\n",
    "d1=json.dumps(all_json)\n",
    "with open('C:/Users/BIG DATA/pythonETL/project/news/2015{}.json'.format(mm), 'w') as f:\n",
    "    f.write(d1) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n"
     ]
    }
   ],
   "source": [
    "print type(all_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
