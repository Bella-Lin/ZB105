{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 1  ---import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import time\n",
    "import re\n",
    "import pymongo\n",
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "from pymongo import MongoClient \n",
    "import findspark\n",
    "import re\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import re\n",
    "import jieba\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import math\n",
    "import time\n",
    "from operator import add\n",
    "\n",
    "#sc = pyspark.SparkContext(appName=\"myAppName\")#myAppName\n",
    "from __future__ import print_function\n",
    "from pyspark import SparkContext\n",
    "#reference  https://www.zybuluo.com/jewes/note/35032\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"myAppName\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 2  ---import dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /home/cloudera/Desktop/project/dics/dict.txt.big.txt ...\n",
      "Loading model from cache /tmp/jieba.ubd40686f1bbfd4d132e1423888a99ba6.cache\n",
      "Loading model cost 0.561 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "jieba.set_dictionary('/home/cloudera/Desktop/project/dics/dict.txt.big.txt')   #切換至中文繁體字庫\n",
    "jieba.load_userdict('/home/cloudera/Desktop/project/dics/dict_keyw.txt')       #加入自建詞庫\n",
    "jieba.load_userdict('/home/cloudera/Desktop/project/dics/dict_keyw_new.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 3  ---get data from mongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1680176\n"
     ]
    }
   ],
   "source": [
    "tag1 = \"\"\n",
    "#tag2 = \":\"\n",
    "#預設就是自己\n",
    "client = MongoClient('10.120.28.6',27017)\n",
    "database = client['Project']\n",
    "collection =database['news']\n",
    "\n",
    "#database = client['test_new1']\n",
    "#collection =database['test1']\n",
    "\n",
    "#querry_resp = collection.find({\"$and\":[\n",
    "#            {\"content\":{\"$regex\":tag1}},\n",
    "           # {\"content\":{\"$regex\":tag2}},\n",
    "#        ]})\n",
    "\n",
    "querry_resp = collection.find().limit(6000)\n",
    "print ((querry_resp).count())\n",
    "#print (type(querry_resp))\n",
    "#print \"查詢結果的第一筆資料標題:\",querry_resp[0][\"title\"]\n",
    "#print \"找到都少筆數:\", querry_resp.count()\n",
    "#print (len(list(querry_resp)))\n",
    "#for i in querry_resp:\n",
    "    #print (i['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共有多少個新聞寫入 6000\n",
      "寫檔案時間 10.46\n"
     ]
    }
   ],
   "source": [
    "write_file_1=time.clock()  \n",
    "\n",
    "i=0\n",
    "with codecs.open(\"/home/cloudera/Desktop/project/practice/project_TFIDF_0520_6000.json\",'w',encoding='utf-8') as f:\n",
    "    for post in querry_resp: \n",
    "        i=i+1\n",
    "        writer=[]\n",
    "        content=[]\n",
    "        #清理文章中的符號\n",
    "        for ele in post['content']:\n",
    "            fil = re.sub(\"[\\s+\\.\\!\\/_,$%^*「」(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\".decode(\"utf8\"), \"\".decode(\"utf8\"),ele[0])\n",
    "            if fil ==\"\":\n",
    "                pass\n",
    "            else:\n",
    "                content.append(ele)\n",
    "                #print (ele)  # show keywords and count\n",
    "        separator=''\n",
    "        joined_content=separator.join(content)  \n",
    "        \n",
    "        #print (joined_content)\n",
    "        writer =  (str(post['_id']) + \"|\" +joined_content)\n",
    "        #writer = ( \"{\" + str(post['_id']) + \":\" + post['content'] + \"}\")\n",
    "        #writer = post['content']\n",
    "        #print  type(summary)\n",
    "        #writer_A=(' '.join(jieba.cut(post['content'])))  #把一篇文章切完詞後 再用 \"/\" 接起來放在一起!    \n",
    "        #writer_B= str(post['_id'])\n",
    "        #writer_c= ( \"{\" + writer_B + \":\" + writer_A + \"}\")\n",
    "        writer1=writer+'\\n'\n",
    "        f.write(writer1) \n",
    "write_file_2=time.clock()  \n",
    "print  (\"共有多少個新聞寫入\" , i) \n",
    "write_file=write_file_2-write_file_1 \n",
    "print  (\"寫檔案時間\" , write_file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 4  ---TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 4-1 ---- 計算總特徵數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "181391\n"
     ]
    }
   ],
   "source": [
    "documents = sc.textFile(\"file:/home/cloudera/Desktop/project/practice/project_TFIDF_0520_6000.json\")\n",
    "collection_count=documents.count()\n",
    "print (collection_count)\n",
    "#每一個元素以flatMap的方式斷詞  ex, (u'\\u6843\\u5712', 1)\n",
    "documents_count_feature=documents.map(lambda line: (line.split(\"|\")[1])).flatMap(lambda x: jieba.cut(x)).map(lambda x :(x,1))\n",
    "documents_count_feature1=documents_count_feature.reduceByKey(lambda x,y : x+y)\n",
    "count_features=documents_count_feature1.count()\n",
    "print (count_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 4-2 ---- 計算TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF-DIY\n",
      "查詢結果資料總筆數: 6000\n",
      "特徵值數量: 181391\n",
      "計算tf-idf的時間 1.89 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "conf = (SparkConf().set(\"spark.driver.maxResultSize\", \"2g\"))\n",
    "# 加大spark輸出結果用的記憶體大小(不是運算用的)\n",
    "#sc = pyspark.SparkContext(master=\"local[*]\", conf=conf, appName=\"kmeans\")\n",
    "\n",
    "time_start_TFIDF= time.clock()\n",
    "\n",
    "#讀取檔案，以\"\\n\"為一個元素\n",
    "documents = sc.textFile(\"file:/home/cloudera/Desktop/project/practice/project_TFIDF_0520_6000.json\")\n",
    "#print (documents.collect())\n",
    "\n",
    "#總共元素數量\n",
    "documents_count=documents.count() \n",
    "#print (type(documents_count))\n",
    "\n",
    "#印出(文章id , keyword)為一個元素的RDD. ex,(u'571787513f3dfa38f08e7009', u'\\u7e3d\\u7d71')\n",
    "wordRDD1 = documents.map(lambda line: (line.split(\"|\")[0],line.split(\"|\")[1]))\\\n",
    ".flatMapValues(lambda x: jieba.cut(x))   #.map(lambda x: ((x[0], x[1]), 1)).reduceByKey(add).groupByKey().mapValues(list)  \n",
    "#print (wordRDD1.collect())\n",
    "\n",
    "#每一篇文章中的總字詞數\n",
    "wordRDD1_totalKeywords_Article=wordRDD1.map(lambda x: ((x[0].encode('utf-8')),1)).reduceByKey(add)\n",
    "#print (wordRDD1_totalKeywords_Article.collect())\n",
    "\n",
    "#一個關鍵字在此篇文章中出現的總次數\n",
    "wordRDD1_wordcount_TF = documents.map(lambda line: (line.split(\"|\")[0].encode('utf-8'),line.split(\"|\")[1]))\\\n",
    ".flatMapValues(lambda x: jieba.cut(x)).map(lambda x: ((x[0].encode('utf-8'), x[1]), 1)).reduceByKey(add).groupByKey().mapValues(list)\n",
    "#print (wordRDD1_wordcount_TF.collect())\n",
    "\n",
    "#一個關鍵字出現在幾篇文章中    \n",
    "wordRDD1_wordcount_IDF = documents.map(lambda line: (line.split(\"|\")[0].encode('utf-8'),line.split(\"|\")[1]))\\\n",
    ".flatMapValues(lambda x: jieba.cut(x)).map(lambda x: ((x[0].encode('utf-8'), x[1]), 1)).reduceByKey(add)\\\n",
    ".groupByKey().mapValues(list).map(lambda x:(x[0][1],1)).reduceByKey(add).groupByKey().mapValues(list)    \n",
    "#print (wordRDD1_wordcount_IDF.collect())\n",
    "\n",
    "#一個關鍵字出現在幾篇文章中的broadcast前置作業\"-------------\"\n",
    "dict_total_word_count=wordRDD1_wordcount_IDF.collect()\n",
    "dict_total_word_count1=dict(dict_total_word_count)\n",
    "#print (dict_total_word_count1)\n",
    "\n",
    "#每一篇文章中的總字詞數broadcast前置作業\"-------------\"\n",
    "dict_total_count=wordRDD1_totalKeywords_Article.collect()\n",
    "dict_total_count1=dict(dict_total_count)\n",
    "#print (dict_total_count1)    #ok\n",
    "\n",
    "#broadcast會將此份字典傳到每一個slave中\n",
    "broadcastVar_1= sc.broadcast(dict_total_count1)\n",
    "broadcastVar_2= sc.broadcast(dict_total_word_count1)\n",
    "\n",
    "#s='571787513f3dfa38f08e700c'\n",
    "#print  (help(broadcastVar))\n",
    "#print (type(s))\n",
    "#print (broadcastVar.value[s])   #ok\n",
    "\n",
    "\n",
    "def my_func_TFIDF(letter):\n",
    "    article_id=letter[0][0]   #文章的id\n",
    "    keyword=letter[0][1]       #keyword\n",
    "    keyword_count_article=(letter[1])[0]   #一個關鍵字在此篇文章中出現的總次數\n",
    "    words_count_article=int(broadcastVar_1.value[article_id])  #查詢該文章的總字詞數\n",
    "    keywords_count_article=(broadcastVar_2.value[keyword])[0]  #查詢該關鍵字在多少篇文章中出現\n",
    "    TF_keyword=(keyword_count_article)/float(words_count_article) #計算詞頻(TF)\n",
    "    IDF_keyword=(np.log(float(documents_count) / float(keywords_count_article))+ 1.0)  #計算詞頻逆向文件頻率(IDF)\n",
    "    TFIDF_word=TF_keyword*IDF_keyword     #TF*IDF\n",
    "    #test=math.log10(float(3) / float(1) )\n",
    "    #test=np.log(float(3) / float(1))\n",
    "    #IDF_keyword=np.log(documents_count/)\n",
    "    return ((article_id,keyword),TFIDF_word)\n",
    "\n",
    "#print (wordRDD1_wordcount.keys().map(lambda line: (line[0])).collect())    #key\n",
    "#print (wordRDD1_wordcount_IDF.collect())\n",
    "#JustDo_TF=wordRDD1_wordcount.keys().map(lambda line: (line[0])).map(my_func)#.values()\n",
    "\n",
    "#傳入元素樣貌→ (pair keys (文章id , 關鍵字) : value 在此文章中出現的總次數 )\n",
    "# ex.  (('571787513f3dfa38f08e700c', u'\\u9080\\u8acb'), [1])\n",
    "\n",
    "\n",
    "JustDo_TFIDF=wordRDD1_wordcount_TF.map(lambda x: my_func_TFIDF(x))#.values()  \n",
    "#print (JustDo_TFIDF.collect())             \n",
    "\n",
    "\n",
    "#TFIDF_list=JustDo_TFIDF.collect()\n",
    "#print (TFIDF_list)\n",
    "print (\"TFIDF-DIY\")\n",
    "print (\"查詢結果資料總筆數:\" ,collection_count)\n",
    "print (\"特徵值數量:\" , count_features)\n",
    "\n",
    "\n",
    "TFIDF_list=JustDo_TFIDF.collect()\n",
    "time_end_TFIDF= time.clock()        \n",
    "time_for_TFIDF= time_end_TFIDF-time_start_TFIDF\n",
    "print (\"計算tf-idf的時間\" , time_for_TFIDF,\"seconds\")\n",
    "#print (len(TFIDF_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 5  ---演算結果印出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查詢結果資料總筆數: 32897\n",
      "特徵值數量: 451056\n",
      "計算tf-idf的時間 0.03 seconds\n"
     ]
    }
   ],
   "source": [
    "print (\"查詢結果資料總筆數:\" ,collection_count)\n",
    "print (\"特徵值數量:\" , count_features)\n",
    "print (\"計算tf-idf的時間\" , time_for_TFIDF,\"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.64\n",
      "75.61\n"
     ]
    }
   ],
   "source": [
    "print (time_end_TFIDF)\n",
    "print (time_start_TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TFIDF_list=JustDo_TFIDF.collect()\n",
    "print (len(TFIDF_list))\n",
    "\n",
    "#TFIDF_list.saveAsTextFile(\"file:/home/cloudera/Desktop/project/output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TFIDF_list=JustDo_TFIDF.collect()\n",
    "for i in TFIDF_list:\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HashTF, IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220604\n"
     ]
    }
   ],
   "source": [
    "##count features\n",
    "documents = sc.textFile(\"file:/home/cloudera/Desktop/project/practice/project_TFIDF_0514.json\")\n",
    "collection_count=documents.count()\n",
    "\n",
    "documents_count_feature=documents.map(lambda line: (line.split(\"|\")[1]) ).flatMap(lambda x: jieba.cut(x)).map(lambda x :(x,1))\n",
    "documents_count_feature1=documents_count_feature.reduceByKey(lambda x,y : x+y)\n",
    "count_features=documents_count_feature1.count()\n",
    "print (count_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查詢結果資料總筆數: 6000\n",
      "hashTF,IDF\n",
      "特徵值數量: 181391\n",
      "計算tf-idf的時間 1.95 seconds\n"
     ]
    }
   ],
   "source": [
    "time_start_TFIDF\n",
    "documents = sc.textFile(\"file:/home/cloudera/Desktop/project/practice/project_TFIDF_0520_6000.json\")\n",
    "collection_count=documents.count()\n",
    "#print (collection_count)\n",
    "\n",
    "print  (\"查詢結果資料總筆數:\" ,collection_count)\n",
    "\n",
    "\n",
    "time_start_jieba = time.clock()\n",
    "documents1=documents.map(lambda line: (line.split(\"|\")[0],line.split(\"|\")[1])).flatMapValues(lambda x: jieba.cut(x)).groupByKey().mapValues(list).map(lambda x: ( x[1]))\n",
    "#print (documents1.collect())\n",
    "time_end_jieba = time.clock()\n",
    "\n",
    "#print (documents1.collect())\n",
    "\n",
    "time_for_jieba= time_end_jieba-time_start_jieba\n",
    "\n",
    "#print (\"I/O time + jieba的時間:\" ,time_for_jieba,\"seconds\")\n",
    "print (\"hashTF,IDF\")\n",
    "print (\"特徵值數量:\" , count_features)\n",
    "\n",
    "#print (documents.collect())\n",
    "#wordRDD1 = documents.flatMapValues(lambda x: jieba.cut(x))#.map(lambda x: ((x[0], x[1]), 1)).reduceByKey(add)#.groupByKey().mapValues(list)     #append('/'.join(jieba.cut(summary)))                                                                                                                                                                                                                                                                           \n",
    "#wordRDD2 =wordRDD1.groupByKey().mapValues(list)\n",
    "#wordRDD3=wordRDD2.map(lambda x: ( x[1]))\n",
    "#print (wordRDD2.collect())\n",
    "\n",
    "#time_start_TFIDF = time.clock()\n",
    "hashingTF = HashingTF()\n",
    "tf = hashingTF.transform(documents1)\n",
    "tf.cache()\n",
    "#print (\"--------------------------\")\n",
    "#print (\"tf:\")\n",
    "#print (tf.collect()) \n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf)   #<class 'pyspark.mllib.feature.IDFModel'>\n",
    "time_end_TFIDF= time.clock()\n",
    "\n",
    "time_for_TFIDF= time_end_TFIDF-time_start_TFIDF\n",
    "print (\"計算tf-idf的時間\" , time_for_TFIDF,\"seconds\")\n",
    "\n",
    "#print (\"--------------------------\")\n",
    "#print (\"tfidf:\")\n",
    "#print (tfidf.collect()) \n",
    "\n",
    "#combined = documents.zip(tfidf)\n",
    "#print (\"--------------------------\")\n",
    "#print (combined.collect()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
